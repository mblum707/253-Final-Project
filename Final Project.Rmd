---
title: "253 Final Project"
author: "Melissa Blum, Andrew Padgett, Tomas Panek"
output: html_document
---

#Loading Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r, echo=FALSE}
#plotting and exploring
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models
library(rpart.plot) #for plotting decision trees
library(vip) #for importance plots
library(pROC) #for ROC curves
library(plotROC) #for plotting ROC curves

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output
library(janitor) #for nice names

#data
library(ISLR) #for data
library(moderndive) #for data
library(rattle) #weather data

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of transformations
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(rpart) #for decision trees
library(randomForest) #for bagging and random forests

theme_set(theme_minimal())
```

#Introduction

In our final project, we are going to explore the database of almost 70,000 inpatient diabetes encounters. The database has been provided by Center for Clinical and Translational Research at Virginia Commonwealth University. Each of the patient record within the dataset includes a wealth of information about their health and physical characteristics, along with the resulting status whether they have been readmitted to the hospital some time after their first visit.

We are going to construct statistical models that try to predit the readmission of a patient. There has been one paper that used the same data: **Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records** by *Strack et al*. They have used a logistic regression model primarily focusing on the measurement HbA1c and its effect on rate of readmission. We think it would be excellent to explore this dataset more in detail and try creating statistical models for making accurate predictions of readmission.

#Data Analysis

*Data Source: [archive.ics.uci.edu/ml/machine-learning-databases/00296/](archive.ics.uci.edu/ml/machine-learning-databases/00296/)*

There are total of 101,766 observations of 50 variables.

```{r}
# loading the dataset
diabetic_data <- read_csv("diabetic_data.csv", na = "?")
# getting the number of observations and variables
dim(diabetic_data)
# getting a summary of the variables
names(diabetic_data)
# head of the dataset
head(diabetic_data)
```

Links to data and paper:

http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008#
https://www.hindawi.com/journals/bmri/2014/781670/

Table using count function for medical specialty diag 

```{r}
diabetic_data %>% count(medical_specialty) %>% arrange(n)

```

```{r}

# Perform logistic regression
med_spec <- train(
    as.factor(readmitted1) ~ medical_specialty,
    data = diabetic_data %>%
        mutate(readmitted1 = (readmitted != "no")),
    method = "glm",
    family = "binomial",
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r}
summary(med_spec)
```
ok to drop

create new variables for particular diagnostic codes

428, 414, 427
276, 250, 401, 496, 403, 

```{r}
diabetic_data %>% count(diag_1) %>% arrange(desc(n))
```

```{r}
diabetic_data %>% count(diag_2) %>% arrange(desc(n))
```

```{r}
diabetic_data %>% count(diag_3) %>% arrange(desc(n))
```

```{r, fig.height = 18, fig.width = 10}
diabetic_data %>% 
  select(-encounter_id, -patient_nbr, -medical_specialty, -diag_1, -diag_2, -diag_3) %>% 
  select_if(is.character) %>% 
  pivot_longer(cols = everything(),names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_bar() +
    scale_y_log10(breaks = scales::trans_breaks("log10",
                                               function(x) 10^x),
                 labels = scales::comma) + 
  # annotation_logticks(sides = "l") +
    coord_flip()+
    facet_wrap(ncol = 2, vars(variable), scales = "free")
```

Variables to exclude: troglitazone, tolbutamide, tolazamide, rosiglatazone, nateglinide, acetohexamide, metformin-piag, citoglipton, examide, 

```{r}
diabetic_data %>% 
  select(-encounter_id, -patient_nbr) %>% 
  select_if(is.numeric) %>% 
  pivot_longer(cols = everything(),names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(vars(variable), scales = "free")
```


```{r}
diabetic_edited_diag <- diabetic_data %>%
  mutate(diag_1_new = fct_lump(diag_1, n = 5),
         diag_2_new = fct_lump(diag_2, n = 5),
         diag_3_new = fct_lump(diag_3, n = 5),
         admission_type_id = as.factor(admission_type_id),
         admission_source_id = as.factor(admission_source_id),
         discharge_disposition_id = as.factor(discharge_disposition_id),
         readmitted_30 = ifelse(readmitted == "<30", 1, 0)) %>%
  select(-weight, -payer_code, -encounter_id, -patient_nbr, -troglitazone, -tolbutamide, -tolazamide, -rosiglitazone, -repaglinide, -nateglinide, -acetohexamide, -`metformin-pioglitazone`, -citoglipton, -examide, -miglitol, -`metformin-rosiglitazone`,-`glimepiride-pioglitazone`, -`glipizide-metformin`,-chlorpropamide, -acarbose, -diag_1, -diag_2, -diag_3, -readmitted, -medical_specialty, -`glyburide-metformin`) %>%
  drop_na()


```


#Model Fitting

```{r}
set.seed(253)

diabetic_split_diag <- diabetic_edited_diag %>% 
  initial_split(prop = .5, strata = readmitted_30)

diag_train <- training(diabetic_split_diag)
diag_test <-testing(diabetic_split_diag)

```


##Logistic Regression

```{r}
# Perform logistic regression
log_reg <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r}
# Perform logistic regression
log_reg_sample <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    metric = "Accuracy",
    na.action = na.omit
)
```

##Lasso Logistic 

```{r}
set.seed(253)

lambda_grid <- 10^seq(-4, -2, length = 100)

log_lasso <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_train,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = lambda_grid),
    metric = "Accuracy",
    na.action = na.omit
)

```

```{r}
set.seed(253)

lambda_grid <- 10^seq(-4, -1, length = 300)

log_lasso_sample <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_train,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = lambda_grid),
    metric = "Accuracy",
    na.action = na.omit
)

```

##Classification Tree

```{r}
set.seed(253)

cp_grid <- 10^seq(-6, -2, length = 100)

class_tree <- train(
  as.factor(readmitted_30)~.,
  data = diag_train,
  method = "rpart",
  tuneGrid = data.frame(cp = cp_grid),
  trControl = trainControl(method = "cv", number = 5),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
set.seed(253)

cp_grid <- 10^seq(-4, -2, length = 100)

class_tree_sample <- train(
  as.factor(readmitted_30)~.,
  data = diag_train,
  method = "rpart",
  tuneGrid = data.frame(cp = cp_grid),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```

##Random Forest

```{r}
set.seed(253)

mtry_grid <- seq(2, 102, length = 10)

rand_for <- train(
  as.factor(readmitted_30) ~ .,
  data = diag_train, 
  method = "rf",
  trControl = trainControl(method = "oob"),
  tuneGrid = data.frame(mtry = mtry_grid),
  ntree = 100, 
  importance = TRUE,
  nodesize = 5, 
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
set.seed(253)

# mtry_grid <- seq(2, __, length = __)

rand_for_sample <- train(
  as.factor(readmitted_30) ~ .,
  data = diag_train, 
  method = "rf",
  trControl = trainControl(method = "oob", sampling = "down"),
  tuneGrid = data.frame(mtry = c(2)),
  ntree = 100, 
  importance = TRUE,
  nodesize = 5, 
  metric = "Accuracy",
  na.action = na.omit
)
```

#Model Evaluation on Training Data

No information rate = 88.935%
```{r}
43412/(43412+5401)
```


###Not Downsampled

##Logistic Regression

```{r}
summary(log_reg)
log_reg$results
```

```{r}
probsTest <- predict(log_reg, diag_train, type = "prob")
threshold <- 0.2
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(log_reg, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(log_reg, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Lasso Logistic

```{r}
log_lasso$bestTune
log_lasso$results
```

```{r}
coef(log_lasso$finalModel, 0.00129155)
```

```{r}
probsTest <- predict(log_lasso, diag_train, type = "prob")
threshold <- 0.1
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(log_lasso, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(log_lasso, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Classification Tree

```{r}
class_tree$bestTune
class_tree$results
```

```{r}
vip(class_tree$finalModel, num_features = 16, bar = FALSE)
```

```{r}
rpart.plot(class_tree$finalModel)
```

```{r}
classifications <- predict(class_tree, newdata = diag_train, type = "raw")
confusionMatrix(data = classifications, 
  reference = as.factor(diag_train$readmitted_30), 
  positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(class_tree, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(class_tree, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Random Forest

```{r}
rand_for$bestTune
rand_for$results
```

```{r}
vip(rand_for$finalModel, num_features = 16, bar = FALSE)
```

```{r}
probsTest <- predict(rand_for, diag_train, type = "prob")
threshold <- 0.01
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(rand_for, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(rand_for, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

###Downsampled

##Logistic Regression

```{r}
summary(log_reg_sample)
log_reg_sample$results
```

```{r}
probsTest <- predict(log_reg_sample, diag_train, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")

```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(log_reg_sample, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(log_reg_sample, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Lasso Logistic

```{r}
log_lasso_sample$bestTune
log_lasso_sample$results
```

```{r}
coef(log_lasso_sample$finalModel, 0.1)
```

```{r}
probsTest <- predict(log_lasso_sample, diag_train, type = "prob")
threshold <- 0.1
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(log_lasso_sample, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(log_lasso_sample, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Classification Tree

```{r}
class_tree_sample$bestTune
class_tree_sample$results
```

```{r}
vip(class_tree_sample$finalModel, num_features = 16, bar = FALSE)
```

```{r}
rpart.plot(class_tree_sample$finalModel)
```

```{r}
probsTest <- predict(class_tree_sample, diag_train, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(class_tree_sample, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(class_tree_sample, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Random Forest

```{r}
rand_for_sample$bestTune
rand_for_sample$results
```

```{r}
vip(rand_for_sample$finalModel, num_features = 16, bar = FALSE)
```

```{r}
probsTest <- predict(rand_for_sample, diag_train, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead = predict(rand_for_sample, newdata = diag_train, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(rand_for_sample, newdata = diag_train, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

#Model Evaluation on Testing Data

## Logistic Regression testing (the little arrow things minimize the code-- use alt+l to make them)


### without downsampling
```{r}
# without downsampling
log_reg_test <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_test,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)
```

### with downsampling
```{r}
# with downsampling
log_reg_sample_test <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_test,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    metric = "Accuracy",
    na.action = na.omit
)
```


## Lasso testing


### without downsampling
```{r}
#without downsampling
set.seed(253)

lambda_grid <- 10^seq(-4, -2, length = 100)

log_lasso_test <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_test,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = lambda_grid),
    metric = "Accuracy",
    na.action = na.omit
)

```

### with downsampling
```{r}
# with downsampling
set.seed(253)

lambda_grid <- 10^seq(-4, -1, length = 300)

log_lasso_sample_test <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_test,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = lambda_grid),
    metric = "Accuracy",
    na.action = na.omit
)

```


##Classification Tree testing


### without downsampling
```{r}
# without downsampling
set.seed(253)

cp_grid <- 10^seq(-6, -2, length = 100)

class_tree_test <- train(
  as.factor(readmitted_30)~.,
  data = diag_test,
  method = "rpart",
  tuneGrid = data.frame(cp = cp_grid),
  trControl = trainControl(method = "cv", number = 5),
  metric = "Accuracy",
  na.action = na.omit
)
```

### with downsampling
```{r}
#with downsampling
set.seed(253)

cp_grid <- 10^seq(-4, -2, length = 100)

class_tree_sample_test <- train(
  as.factor(readmitted_30)~.,
  data = diag_test,
  method = "rpart",
  tuneGrid = data.frame(cp = cp_grid),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```


##Random Forest testing


### without downsampling
```{r}
#without downsampling
set.seed(253)

# mtry_grid <- seq(2, __, length = __)

rand_for_test <- train(
  as.factor(readmitted_30) ~ .,
  data = diag_test, 
  method = "rf",
  trControl = trainControl(method = "oob"),
  tuneGrid = data.frame(mtry = c(2)),
  ntree = 100, 
  importance = TRUE,
  nodesize = 5, 
  metric = "Accuracy",
  na.action = na.omit
)
```

### with downsampling
```{r}
#with downsampling
set.seed(253)

# mtry_grid <- seq(2, __, length = __)

rand_for_sample_test <- train(
  as.factor(readmitted_30) ~ .,
  data = diag_test, 
  method = "rf",
  trControl = trainControl(method = "oob", sampling = "down"),
  tuneGrid = data.frame(mtry = c(2)),
  ntree = 100, 
  importance = TRUE,
  nodesize = 5, 
  metric = "Accuracy",
  na.action = na.omit
)
```


###Not Downsampled results

##Logistic Regression testing results

```{r}
summary(log_reg_test)
log_reg_test$results
```

```{r}
probsTest <- predict(log_reg_tets, diag_test, type = "prob")
threshold <- 0.2
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(log_reg_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(log_reg_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Lasso Logistic testing results

```{r}
log_lasso_test$bestTune
log_lasso_test$results
```

```{r}
coef(log_lasso_test$finalModel, 0.003274549)
```

```{r}
probsTest <- predict(log_lasso_test, diag_test, type = "prob")
threshold <- 0.1
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(log_lasso_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(log_lasso_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Classification Tree testing results

```{r}
class_tree_test$bestTune
class_tree_test$results
```

```{r}
# I can't remember if you need to change anything for testing here
vip(class_tree_test$finalModel, num_features = 16, bar = FALSE)
```

```{r}
rpart.plot(class_tree_test$finalModel)
```

```{r}
classifications <- predict(class_tree_test, newdata = diag_test, type = "raw")
confusionMatrix(data = classifications, 
  reference = as.factor(diag_test$readmitted_30), 
  positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(class_tree_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(class_tree_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Random Forest testing results

```{r}
rand_for_test$bestTune
rand_for_test$results
```

```{r}
#not sure if I need to change anything for testing here
vip(rand_for_test$finalModel, num_features = 16, bar = FALSE)
```

```{r}
probsTest <- predict(rand_for, diag_test, type = "prob")
threshold <- 0.01
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(rand_for_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(rand_for_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

###Downsampled

##Logistic Regression downsampled testing results

```{r}
summary(log_reg_sample_test)
log_reg_sample_test$results
```

```{r}
probsTest <- predict(log_reg_sample_test, diag_test, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")

```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(log_reg_sample_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(log_reg_sample_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Lasso Logistic downsampled testing results

```{r}
log_lasso_sample_test$bestTune
log_lasso_sample_test$results
```

```{r}
coef(log_lasso_sample_test$finalModel, 0.02806471	)
```

```{r}
probsTest <- predict(log_lasso_sample_test, diag_test, type = "prob")
threshold <- 0.1
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(log_lasso_sample_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(log_lasso_sample_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Classification Tree testing results

```{r}
class_tree_sample_test$bestTune
class_tree_sample_test$results
```

```{r}
vip(class_tree_sample_test$finalModel, num_features = 16, bar = FALSE)
```

```{r}
rpart.plot(class_tree_sample_test$finalModel)
```

```{r}
probsTest <- predict(class_tree_sample_test, diag_test, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(class_tree_sample_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(class_tree_sample_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Random Forest testing results

```{r}
rand_for_sample_test$bestTune
rand_for_sample_test$results
```

```{r}
vip(rand_for_sample_test$finalModel, num_features = 16, bar = FALSE)
```

```{r}
probsTest <- predict(rand_for_sample_test, diag_test, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead = predict(rand_for_sample_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

```{r}
diag_test %>%
  mutate(PredRead =  predict(rand_for_sample_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

I'm not totally sure I'm remembering this right, but I think we're supposed to test the model that was fit on the training data using the testing data, instead of fitting the same type of model to the testing data and then testing it on the testing data. With the first option, we can detect overfitting and see how the model would perform on a "new" dataset, whereas the second option does sort of the same thing we did when we trained the models and then tested their accuracy on training data. I have edited your code below to do the first option, but I'm leaving everything you did in case I'm wrong about how to do this. 

#Model Evaluation (of model built on Training Data) on Testing Data

###Not Downsampled

##Logistic Regression

```{r}
probsTest <- predict(log_reg, diag_test, type = "prob")
threshold <- 0.2
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(log_reg, newdata = diag_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```


```{r}
diag_test %>%
  mutate(PredRead =  predict(log_reg, newdata = diag_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Lasso Logistic 

```{r}
probsTest <- predict(log_lasso, diag_test, type = "prob")
threshold <- 0.1
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(log_lasso, newdata = diag_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(log_lasso, newdata = diag_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Classification Tree 

```{r}
classifications <- predict(class_tree, newdata = diag_test, type = "raw")
confusionMatrix(data = classifications, 
  reference = as.factor(diag_test$readmitted_30), 
  positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(class_tree, newdata = diag_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(class_tree, newdata = diag_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Random Forest

```{r}
probsTest <- predict(rand_for, diag_test, type = "prob")
threshold <- 0.01
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(rand_for, newdata = diag_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(rand_for, newdata = diag_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```


###Downsampled

##Logistic Regression 

```{r}
probsTest <- predict(log_reg_sample, diag_test, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")

```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(log_reg_sample, newdata = diag_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(log_reg_sample, newdata = diag_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Lasso Logistic 

```{r}
probsTest <- predict(log_lasso_sample, diag_test, type = "prob")
threshold <- 0.1
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(log_lasso_sample, newdata = diag_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(log_lasso_sample, newdata = diag_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Classification Tree 

```{r}
probsTest <- predict(class_tree_sample, diag_test, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead =  predict(class_tree_sample, newdata = diag_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_test %>%
  mutate(PredRead =  predict(class_tree_sample, newdata = diag_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Random Forest

```{r}
probsTest <- predict(rand_for_sample, diag_test, type = "prob")
threshold <- 0.5
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_test$readmitted_30), positive = "1")
```

```{r}
diag_test %>% 
  mutate(PredRead = predict(rand_for_sample, newdata = diag_test, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")
```

```{r}
diag_test %>%
  mutate(PredRead =  predict(rand_for_sample, newdata = diag_test, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```