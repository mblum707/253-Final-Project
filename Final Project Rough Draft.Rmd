---
title: "Modeling Hospital Readmission of Diabetic Patients"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```


Introduction

Diabetes is a chronic disease which affects an alarmingly high proportion of the U.S. population. In 2018, 34.2 million people of all ages in the U.S. (or 10.5% of the population) had received a diabetes diagnosis, and an additional 7.3 million adults were estimated to have undiagnosed diabetes (1). Patients with diabetes experience high blood sugar, which can lead to diabetic emergencies like ketoacidosis if left untreated. Diabetes patients who use insulin to manage their blood sugar levels can also require emergency medical care if a dose of insulin lowers their blood sugar too far. In either of these cases, patients may need to be admitted to a hospital and monitored for several days or weeks before they can return to daily life. 

Patients who repeatedly require hospital admission are exposed to the risks of extended hospital stays, such as blood clots from lying in bed and contact with infectious diseases. Because hospital visits pose risks to patients and are generally more costly to patients than preventative care (regular blood glucose monitoring, modifications to diet and exercise, diabetic medications), modeling that can accurately inform physicians about which patients are most likely to be readmitted to the hospital is valuable. Patients who are more likely to be readmitted can be provided with additional hospital resources upon discharge, such as a health care worker who vists their home once a week to check on their general health, medication compliance, and blood glucose levels. If these resources are directed effectively, it may be possible to prevent hospital readmission for the most at-risk patients by improving their ability to manage their disease. 

Using a dataset that describes 100,000 hospital admissions of diabetic patients in the U.S., we attempt to build several models predicting which patients are most likely to be readmitted to the hospital within 30 days of an initial hospital stay (2). A previous study using the same dataset found that the probability of readmission was significantly influenced by whether or not a laboratory test called HbA1C was administered, and whether the patient's medications were changed if the test result was abnormal (3). If this test, which gives health care providers a sense of how high the patient's blood sugar has been over the last few months, was administered, the patient was less likely to be readmitted, and this effect was even stronger if the patient's medications were changed during the hospital stay after an abnormal HbA1C result. The authors of this study argue for increased use of the HbA1C test for diabetic patients during hospital admissions, but they fail to provide a measure of how accurate their model predicting patient readmission is. 

Here, we explore the ability of several different modeling techniques to predict patient readmission, and we compare measures of modeling accuracy to determine which model(s) could most effectively inform the targeted distribution of hospital resources to at-risk patients. SHORT DESCRIPTION OF RESULTS. 

The dataset that we use to model hospital readmission for diabetes patients contains information on 100,000 patient visits to 130 hospitals in the United States between 1999 and 2008. During each of these visits, a diabetic patient was admitted to the hospital for 1 to 14 days, received laboratory tests, and was administered medication. For each visit, the dataset also provides information on patient demographics, aspects of the patient's stay in the hospital (tests performed and medications administered, length of stay, specialty of the physician who admitted the patient), and the patent's medical history (diagnoses, number of visits to other several types of medical providers during the year before hospitalization). Using these variables, we sought to model the likelihood that patients would be readmitted to the hospital within 30 days of the recorded admission.



Exploratory Plots



We made several data cleaning decisions to facilitate the modeling process. For the variables describing patient diagnoses, we decided to consider the impact of only the five most common diagnoses on readmission. We also removed from the dataset several variables with values largely unique to each observation (unique identifiers for each patient and each hospital visit), several variables with an unmanageable number of levels (the doctor's medical specialty and a code indicating how the patient paid for their admission), and one variable for which most of the values were missing (weight). 

This left us with variables describing patient demographics, test results, medications, and other aspects of the patient's stay in the hospital. From this pool of variables, we eliminated many of the variables describing whether or not a patient took a specific medication or changed their dose of a specific medication because the vast majority of the observations fell into one level of the variable. Variables for which most of the observations have the same value are not very useful for making predictions because they lack variation. After eliminating these variables, we dropped all remaining observations with missing values for any variable, and we were left with 98053 observations. While the dataset readmission variable indicated whether the patient was readmitted within 30 days, readmitted after greater than 30 days, or not readmitted during the study period, we decided to condense two of these categories (>30 days and not readmitted) into a single category in order to use modeling techniques that could make YES/NO predictions as to whether each patient would be readmitted to the hospital within 30 days.

```{r, echo = FALSE }
diabetic_edited_diag <- diabetic_data %>%
  mutate(diag_1_new = fct_lump(diag_1, n = 5),
         diag_2_new = fct_lump(diag_2, n = 5),
         diag_3_new = fct_lump(diag_3, n = 5),
         admission_type_id = as.factor(admission_type_id),
         admission_source_id = as.factor(admission_source_id),
         discharge_disposition_id = as.factor(discharge_disposition_id),
         readmitted_30 = ifelse(readmitted == "<30", 1, 0)) %>%
  select(-weight, -payer_code, -encounter_id, -patient_nbr, -troglitazone, -tolbutamide, -tolazamide, -rosiglitazone, -repaglinide, -nateglinide, -acetohexamide, -`metformin-pioglitazone`, -citoglipton, -examide, -miglitol, -`metformin-rosiglitazone`,-`glimepiride-pioglitazone`, -`glipizide-metformin`,-chlorpropamide, -acarbose, -diag_1, -diag_2, -diag_3, -readmitted, -medical_specialty, -`glyburide-metformin`) %>%
  drop_na()
```

We split the dataset into two evenly sized training and testing datasets in order to validate model performance on a different dataset than the one used to fit each model. This gave us a sense of how each model might perform on a new dataset, which is how it would be used in a clinical setting. Because most patients in the dataset were not readmitted to the hospital within 30 days (those who were are referred to as "YES" observations from here on out), we stratified the testing and training datasets by readmission to ensure an even distribution of YES observations between the two datasets.

We then fit a series of models to the training data, adjusted tuning parameters of each model to achieve the best fit, and evaluated which models might produce the most accurate predictions of readmidssion within 30 days. The types of models we applied include logistic regression, LASSO logistic regression, classification trees, and random forest. Each of these models predicts the probability that a patient will be readmitted to the hospital within 30 days, and the differences between these types of models lie in what assumptions and methods they use to reach these predictions. Each model's probability predictions are then converted to a YES/NO classification based on a probability threshold. For example, if a model's probability predictions are converted to classifications using a threshold of 0.5, than any patient for which the predicted probability of readmission is >0.5 will be classified as a YES, and any patient for which the predicted probabiltiy of readmission is <0.5 will be classified as a NO. 

Once the probability predictions have been converted to classifications, we can compare those classifications to the actual value of the readmission variable for each patient to see how accurate the model is. We use several different measures of accuracy to assess how useful the model would be: accuracy, sensitivity, and specificity. Accuracy is the most intuitive of the three measures in that it describes the percentage of the time that the model's prediction matches the actual value of the readmission variable. Sensitivity is also known as the true positive rate, and it describes the percentage of the time that the model predicts a patient will be readmitted when they really were readmitted. Specificity (the true negative rate) describes the percentage of the time the model predicts a patient will not be readmitted when they really were not. Importantly, there is often a tradeoff between sensitivity and specificity, such that if we do something to the model to increase one of them, we are probably doing so at the expense of the other. 

Ideally, we would just pick the model with the highest accuracy value as our best model and use that one to predict which patients should get extra services. However, if this highly accurate model happened to have high specifity and low sensitivity, we could actually end up providing most of our extra hospital resources to patients who do not really need them. So, instead of just looking at accuracy, we need to think about how each model's sensitivity and specificity interact with the resources available to the hospital. For example, a hospital with a lot of available resources might prefer to accept low specificity in order to achieve high sensitivity, in which case they would provide additional health care services to most of the patients who actually need them to prevent readmission, and also some number of patients who do not really need them. If the hospital's resources are scarce, they might prefer a model with high specificity and low sensitivity, in which case a fair number of patients likely to be readmitted would not receive extra care, but they would also avoid giving extra resources to patients who did not need them. 

In evaluating the performance of the four model types we decided to try, we noticed that they all had fairly low sensitivity at a probability threshold of 0.5, owing to the small proportion of YES observations out of the total number of observations. Since a higher sensitivity might be desirable for the reasons stated above, we explored two strategies for circumventing this problem: (1) adjusting the probability thresholds of the models to increase the sensitivity (usually at the expense of both specificity and accuracy) and (2) artifically changing the distribution of YES and NO observations in the data used to fit the model so that their proportions were more equal. After employing both of these strategies, we compiled a list of the model and the metrics representating their performance on the data that was used to train them. 

Model | Accuracy | Sensitivity | Specificity
--------------|-------------------|------------------|-----------------
Logistic Regression
LASSO 
Classification Tree
Random Forest
Downsampled Logistic Regression
Downsampled LASSO
DOwnsampled Classification Tree
Downsampled Random Forest

From this list, we picked three models that seemed to be performing the best in terms of their accuracy and the balance between sensitivity and specificity (#DISCUSS THIS MORE?), and we tested their performance on the testing data that we had set aside at the beginning of the modeling process. The accuracy metrics produced by this test give us a sense of how each model might perform using new patient data, since none of the models have "seen" the testing data before.

Model | Accuracy | Sensitivity | Specificity
--------------|-------------------|------------------|-----------------












References:

(1) https://www.cdc.gov/diabetes/pdfs/data/statistics/national-diabetes-statistics-report.pdf
(2) http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008
(3) https://www.hindawi.com/journals/bmri/2014/781670/



Lisa's instructions:

Final Report
This will highlight some of the new statistical machine learning techniques you learned in this course and also o show that you can communicate the results to a non-statistician. It should read more like a data journalism article than a formal research paper. Here is one good example. You could also look to fivethirtyeight or The Pudding (my personal favorite) for more good examples.

You should make sure to include the following:

Introduce your data and research question. It might be helpful to include a graph/table that summarizes the response variable. Lay out the plan you will use to analyze/model so we (the readers!) can more easily follow along and knows what to expect. It can be helpful to elude to some of the results at the beginning so we know what to watch for. Also mention any important data cleaning decisions you made. You don’t need to tell us EVERYTHING you do, though.

Exploratory work and basic models. Use graphs to illustrate interesting relationshipos that play a role in your final model. DO NOT just show a bunch of graphs because you can. You should label and discuss every graph you include. There is no required number to include. The graphs should be helping us to understand something about your final model and should help us engage more with the data.

Describe the modeling process. I don’t want to know EVERYTHING you tried. You might have tried 10 different things and in the end chose the 8th one. If you think it is important, you can summarize some other methods you tried. But focus on the analysis you found to be most important. This should include at least one of the techniques you learned in this course. Some essentials you will need in the modeling process:

Use rsample() to split the data into a training and test set.
Tune the model using the training data.
Evaluate model using training data (cross-validation or OOB).
Pick a few “best” models and apply them to the test data to decide on the final model.
Summarize the results succinctly. Reiterate why we should be interested in this analysis. Depending on the project, you might do this in different ways. Maybe there were important relationships you learned about. Or maybe you have a nice way to predict something useful. Let us know what we’ve learned and why it’s important/neat/interesting.

Requirements and tips
Your final product will be a knitted html file. The yaml header (at the very top of the .rmd file between the two sets of three dashes) should be similar to the one I have below. You will also submit the .rmd in a separate place. I will likely not look at that file unless there is something I do not understand.
title: "Title"
output:
  html_document:
    df_print: paged
Only include absolutely essential R code! I want to be able to read through the paper nicely. So, most of your R code chunks should have the echo=FALSE option (or do it for all code chunks using my suggestion in the next bullet). The code will still run, but the code is omitted from the document. What is essential? Probably only the code you use to fit models. Anything code for graphs and evaluation can be omitted. Even some of your modeling code might be able to be omitted. When you include code, that means you find it so essential that you should talk about it.

Use other useful code chunk options. You can add results='hide' to also omit the output. This is good for chunks of code you might want to hang on to in case you decide to use it later but don’t want to use in the paper. message=FALSE and warning=FALSE omit any messages and warnings. This link provides some other basic code chunk options. If you want to apply some of these to your entire document, add a chunk similar to the one below. The options you put in there will be applied to all code chunks.

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
Make your R output look nice.

Label graphs (with nice axis labels too) and tables
The kable() function is good to use for tables.
You can change figure widths and heights in code chunk options using fig.width and fig.height.
Always leave an empty row (push enter twice) after an R code chunk. It should prevent your text from showing up next to a graph or table in an odd way.
Be careful if you make any bulleted/numbered lists. Make sure they look the way you expect when knitted. Leaving an empty row between bullets/numbers often helps.
Interpret the R output! You should be describing what it is and what parts of it are interesting.

There is no length requirement. I recommend making it as short as possible while still accomplishing all the required tasks. See the grading rubric on the moodle page (coming soon).