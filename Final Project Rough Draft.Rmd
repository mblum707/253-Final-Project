---
title: "Modeling Hospital Readmission of Diabetic Patients"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```

```{r, echo=FALSE}
#plotting and exploring
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models
library(rpart.plot) #for plotting decision trees
library(vip) #for importance plots
library(pROC) #for ROC curves
library(plotROC) #for plotting ROC curves

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output
library(janitor) #for nice names

#data
library(ISLR) #for data
library(moderndive) #for data
library(rattle) #weather data

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of transformations
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(rpart) #for decision trees
library(randomForest) #for bagging and random forests

theme_set(theme_minimal())
```

Diabetes is a chronic disease which affects an alarmingly high proportion of the U.S. population. In 2018, 34.2 million people of all ages in the U.S. (or 10.5% of the population) had received a diabetes diagnosis, and an additional 7.3 million adults were estimated to have undiagnosed diabetes (1). Patients with diabetes experience high blood sugar, which can lead to diabetic emergencies like ketoacidosis if left untreated. Diabetes patients who use insulin to manage their blood sugar levels can also require emergency medical care if a dose of insulin lowers their blood sugar too far. In either of these cases, patients may need to be admitted to a hospital and monitored for several days or weeks before they can return to daily life. Moreover, uncontrolled diabetes may put patients at risk of developing other diseases and affect the prognosis of other diseases.

Patients who repeatedly require hospital admission are exposed to the risks of extended hospital stays, such as blood clots from lying in bed and contact with infectious diseases. Because hospital visits pose risks to patients and are generally more costly to patients than preventative care (regular blood glucose monitoring, modifications to diet and exercise, diabetic medications), modeling that can accurately inform physicians about which patients are most likely to be readmitted to the hospital is valuable. Patients who are more likely to be readmitted can be provided with additional hospital resources upon discharge, such as a health care worker who vists their home once a week to check on their general health, medication compliance, and blood glucose levels. If these resources are directed effectively, it may be possible to prevent hospital readmission for the most at-risk patients by improving their ability to manage their disease. 

Using a dataset that describes 100,000 hospital admissions of diabetic patients in the U.S., we attempt to build several models predicting which patients are most likely to be readmitted to the hospital within 30 days of an initial hospital stay (2). A previous study using the same dataset found that the probability of readmission was significantly influenced by whether or not a laboratory test called HbA1C was administered, and whether the patient's medications were changed if the test result was abnormal (3). If this test, which gives health care providers a sense of how high the patient's blood sugar has been over the last few months, was administered, the patient was less likely to be readmitted, and this effect was even stronger if the patient's medications were changed during the hospital stay after an abnormal HbA1C result. The authors of this study argue for increased use of the HbA1C test for diabetic patients during hospital admissions, but they fail to provide a measure of how accurate their model predicting patient readmission is. 

Here, we explore the ability of several different modeling techniques to predict patient readmission, and we compare measures of modeling accuracy to determine which model(s) could most effectively inform the targeted distribution of hospital resources to at-risk patients. SHORT DESCRIPTION OF RESULTS. 

The dataset that we use to model hospital readmission for diabetes patients contains information on 100,000 patient visits to 130 hospitals in the United States between 1999 and 2008. During each of these visits, a diabetic patient was admitted to the hospital for 1 to 14 days, received laboratory tests, and was administered medication. For each visit, the dataset also provides information on patient demographics, aspects of the patient's stay in the hospital (tests performed and medications administered, length of stay, specialty of the physician who admitted the patient), and the patent's medical history (diagnoses, number of visits to other several types of medical providers during the year before hospitalization). Using these variables, we sought to model the likelihood that patients would be readmitted to the hospital within 30 days of the recorded admission.

```{r}
# loading the dataset
diabetic_data <- read_csv("diabetic_data.csv", na = "?")
head(diabetic_data)
```

We made several data cleaning decisions to facilitate the modeling process. For the variables describing patient diagnoses, we decided to consider the impact of only the five most common diagnoses on readmission. We also removed from the dataset several variables with values largely unique to each observation (unique identifiers for each patient and each hospital visit), several variables with an unmanageable number of levels (the doctor's medical specialty and a code indicating how the patient paid for their admission), and one variable for which most of the values were missing (weight). 

This left us with variables describing patient demographics, test results, medications, and other aspects of the patient's stay in the hospital. From this pool of variables, we eliminated many of the variables describing whether or not a patient took a specific medication or changed their dose of a specific medication because the vast majority of the observations fell into one level of the variable. Variables for which most of the observations have the same value are not very useful for making predictions because they lack variation. After eliminating these variables, we dropped all remaining observations with missing values for any variable, and we were left with 98053 observations. While the dataset readmission variable indicated whether the patient was readmitted within 30 days, readmitted after greater than 30 days, or not readmitted during the study period, we decided to condense two of these categories (>30 days and not readmitted) into a single category in order to use modeling techniques that could make YES/NO predictions as to whether each patient would be readmitted to the hospital within 30 days.

```{r, echo = FALSE }
diabetic_edited_diag <- diabetic_data %>%
  mutate(diag_1_new = fct_lump(diag_1, n = 5),
         diag_2_new = fct_lump(diag_2, n = 5),
         diag_3_new = fct_lump(diag_3, n = 5),
         admission_type_id = as.factor(admission_type_id),
         admission_source_id = as.factor(admission_source_id),
         discharge_disposition_id = as.factor(discharge_disposition_id),
         readmitted_30 = ifelse(readmitted == "<30", 1, 0)) %>%
  select(-weight, -payer_code, -encounter_id, -patient_nbr, -troglitazone, -tolbutamide, -tolazamide, -rosiglitazone, -repaglinide, -nateglinide, -acetohexamide, -`metformin-pioglitazone`, -citoglipton, -examide, -miglitol, -`metformin-rosiglitazone`,-`glimepiride-pioglitazone`, -`glipizide-metformin`,-chlorpropamide, -acarbose, -diag_1, -diag_2, -diag_3, -readmitted, -medical_specialty, -`glyburide-metformin`) %>%
  drop_na()
```



@ANDREW, Exploratory Plots: show and discuss distribution of readmission variable, other important variables as indicated by logistic regression, variable importance plots (e.g. patient disposition, number inpatient, number emergency, etc.). Thanks!

To get better acquainted with the data, we created some exploratory plots. The following plots display the percentage of people that fall into each category that was readmitted to the hospital within 30 days. For the first graph, the discharge disposition that return the highest percentage of people readmitted within 30 days was "still patient or expected to return for outpatient services". This makes sense because these are patients that were likely directly told to return within 30 days. However, no patients that had died returned within 30 days, which also makes sense. The lowest nonzero percentage was for patients transferred to hospice. 

The second graph shows that the number of days that the patient spent in the hospital directly correlates to their likelihood of returning within 30 days. Patients that spent longer times in the hospital were more likely to return that patients who spent shorter amounts of time in the hospital. 

The third graph shows that the number of emergency encounters a patient has seems to correlate with the probability of being readmitted. The data on the right side of the graph is a bit sparese, but the first 20 or so values indicate that the more emergency encounter a patient has, the more likely they are to be readmitted to the hospital within 30 days. 

```{r}
diabetic_edited_diag %>%
  ggplot(aes(x = discharge_disposition_id)) + 
  geom_bar(aes(fill=factor(readmitted_30)), position = "fill")

diabetic_edited_diag %>%
  ggplot(aes(x = number_inpatient)) + 
  geom_bar(aes(fill=factor(readmitted_30)), position = "fill")

diabetic_edited_diag %>%
  ggplot(aes(x = number_emergency)) + 
  geom_bar(aes(fill=factor(readmitted_30)), position = "fill")
```

We split the dataset into two evenly sized training and testing datasets in order to validate model performance on a different dataset than the one used to fit each model. This gave us a sense of how each model might perform on a new dataset, which is how it would be used in a clinical setting. Because most patients in the dataset were not readmitted to the hospital within 30 days (those who were are referred to as "YES" observations from here on out), we stratified the testing and training datasets by readmission to ensure an even distribution of YES observations between the two datasets.

We then fit a series of models to the training data, adjusted tuning parameters of each model to achieve the best fit, and evaluated which models might produce the most accurate predictions of readmidssion within 30 days. The types of models we applied include logistic regression, LASSO logistic regression, classification trees, and random forest. Each of these models predicts the probability that a patient will be readmitted to the hospital within 30 days, and the differences between these types of models lie in what assumptions and methods they use to reach these predictions. Each model's probability predictions are then converted to a YES/NO classification based on a probability threshold. For example, if a model's probability predictions are converted to classifications using a threshold of 0.5, than any patient for which the predicted probability of readmission is >0.5 will be classified as a YES, and any patient for which the predicted probabiltiy of readmission is <0.5 will be classified as a NO. 

Once the probability predictions have been converted to classifications, we can compare those classifications to the actual value of the readmission variable for each patient to see how accurate the model is. We use several different measures of accuracy to assess how useful the model would be: accuracy, sensitivity, and specificity. Accuracy is the most intuitive of the three measures in that it describes the percentage of the time that the model's prediction matches the actual value of the readmission variable. Sensitivity (also known as the true positive rate) describes the percentage of the time that the model predicts a patient will be readmitted when they really were readmitted. Specificity (also known as the true negative rate) describes the percentage of the time the model predicts a patient will not be readmitted when they really were not. Importantly, there is often a tradeoff between sensitivity and specificity, such that if we do something to the model to increase one of them, we are probably doing so at the expense of the other. 

Ideally, we would just pick the model with the highest accuracy value as our best model and use that one to predict which patients should get extra services. However, if this highly accurate model happened to have high specifity and low sensitivity, we could actually end up providing most of our extra hospital resources to patients who do not really need them. So, instead of just looking at accuracy, we need to think about how each model's sensitivity and specificity interact with the resources available to the hospital. For example, a hospital with a lot of available resources might prefer to accept low specificity in order to achieve high sensitivity, in which case they would provide additional health care services to most of the patients who actually need them to prevent readmission, and also some number of patients who do not really need them. If the hospital's resources are scarce, they might prefer a model with high specificity and low sensitivity, in which case a fair number of patients likely to be readmitted would not receive extra care, but the hospital would also avoid giving extra resources to patients who did not need them. 


WHAT IS AUC?


In evaluating the performance of the four model types we decided to try, we noticed that they all had fairly low sensitivity at a probability threshold of 0.5, owing to the small proportion of YES observations out of the total number of observations. Since a higher sensitivity might be desirable for the reasons stated above, we explored two strategies for circumventing this problem: (1) adjusting the probability thresholds of the models to increase the sensitivity (usually at the expense of both specificity and accuracy) and (2) artifically changing the distribution of YES and NO observations in the data used to fit the model so that their proportions were more equal. After employing both of these strategies, we compiled a list of the model and the metrics representating their performance on the data that was used to train them. 

Model | Accuracy | Sensitivity | Specificity | AUC
--------------|-------------------|------------------|-----------------|----------------
Logistic Regression | 0.5858  | 0.67393 | 0.57469 | 0.6724
LASSO | 0.5905 | 0.6667 | 0.5808 | 0.6722
Classification Tree | 0.8877 | 0.0000 | 1.0000 | 0.5
Random Forest | 0.895 | 0.27342 | 0.97369 | 0.5261
Downsampled Logistic Regression | 0.5889 | 0.66013 | 0.57984 | 0.6688
Downsampled LASSO | 0.8877 | 0.0000 | 1.0000 | 0.5
Downsampled Classification Tree | 0.6416 | 0.57988 | 0.64942 | 0.6343
Downsampled Random Forest | 0.8732 | 0.9049 | 0.8692 | 0.9546

From this list, we eliminated two models (Classification Tree and Downsampled LASSO), which predicted that all patients would not be readmitted when the "best" tuning parameters were used to maximize model accuracy. We also identified four models that seemed to be performing the best in terms of their accuracy and the balance between sensitivity and specificity, and we decided to dive deeper into the pros and cons of each model based on its performance on the training data.

@TOMAS, here is the code for fitting and evaluating the three types of models we decided to test on testing data. Feel free to delete/hide code as necessary. Maybe only show code for the model evaluation for one of these models? Thanks! Maybe also mention what effect the "important" variables highlighted in Andrew's exploratory plots have on readmission based on lasso?

##Lasso Logistic (keeping b/c similar accuracy as logistic, but simpler model)

```{r}
set.seed(253)

lambda_grid <- 10^seq(-4, -2, length = 100)

log_lasso <- train(
    as.factor(readmitted_30) ~ .,
    data = diag_train,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = lambda_grid),
    metric = "Accuracy",
    na.action = na.omit
)

```

```{r}
log_lasso$bestTune
log_lasso$results
```

```{r}
coef(log_lasso$finalModel, 0.00129155)
```

```{r}
probsTest <- predict(log_lasso, diag_train, type = "prob")
threshold <- 0.1
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(log_lasso, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(log_lasso, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Downsampled Classification Tree

```{r}
set.seed(253)

cp_grid <- 10^seq(-4, -2, length = 100)

class_tree_sample <- train(
  as.factor(readmitted_30)~.,
  data = diag_train,
  method = "rpart",
  tuneGrid = data.frame(cp = cp_grid),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
vip(class_tree_sample$finalModel, num_features = 16, bar = FALSE)
```

```{r}
rpart.plot(class_tree_sample$finalModel)
```

```{r}
probsTest <- predict(class_tree_sample, diag_train, type = "prob")
threshold <- 0.44
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(class_tree_sample, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(class_tree_sample, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

##Random Forests (we will test both normal and downsampled, but code is included only for normal)

```{r}
set.seed(253)

mtry_grid <- seq(2, 102, length = 10)

rand_for <- train(
  as.factor(readmitted_30) ~ .,
  data = diag_train, 
  method = "rf",
  trControl = trainControl(method = "oob"),
  tuneGrid = data.frame(mtry = mtry_grid),
  ntree = 100, 
  importance = TRUE,
  nodesize = 5, 
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
vip(rand_for$finalModel, num_features = 16, bar = FALSE)
```

```{r}
probsTest <- predict(rand_for, diag_train, type = "prob")
threshold <- 0.01
pred      <- factor( ifelse(probsTest[, "1"] > threshold, "1", "0") )
confusionMatrix(pred, as.factor(diag_train$readmitted_30), positive = "1")
```

```{r}
diag_train %>% 
  mutate(PredRead =  predict(rand_for, type = "prob")$"1") %>%
  ggplot(aes(d = as.integer(readmitted_30), m = PredRead)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray")

diag_train %>%
  mutate(PredRead =  predict(rand_for, type = "prob")$"1") %>%
  roc(readmitted_30 ~ PredRead, data=.) %>%
  auc()
```

Evaluating model performance on the training data is helpful, but it does not tell us the whole story. Sometimes, models suffer from overfitting, which occurs when a model is fit so closely to the training data that it makes highly accurate predictions on the training data, but performs poorly with new data. In order to see whether any of the four models discussed above were overfit, we tested the performance of each of these models on the testing data that we had set aside at the beginning of the modeling process. The accuracy metrics produced by this test give us a sense of how each model might perform using new patient data, since none of the models have "seen" the testing data before. Below is a table displaying the accuracy metrics for each model tested on the testing data. 

Model | Accuracy | Sensitivity | Specificity | AUC
--------------|-------------------|------------------|-----------------|-----------------
LASSO Logistic | 0.5859 | 0.6634 | 0.5760 | 0.6704
Random Forest | 0.8550 | 0.1146 | 0.9497 | 0.5536 
Downsampled Random Forest | 0.6291 | 0.58024 | 0.63539 | 0.6508
Downsampled Classification Tree | 0.6386 | 0.56351 | 0.64815 | 0.6282

We can draw a few conclusions from these metrics. First, it seems like the downsampled random forest model, which performed so well on the training data, was overfit. When this model was used to make predictions on the testing data, its accuracy, sensitivity, specificity, and AUC all dropped considerably. However, the downsampled random forest model still performed comparably to the downsampled classification tree model in terms of all of these metrics. 

These two models, as well as the lasso logistic model, offer similar balances of sensitivity and specificity, which would be well-suited to a hospital with plentiful resources available for allocation. All of these models make correct predictions for a fairly high proportion of patients who are readmitted (~57%), but they also incorrectly predict that about 35% of the actual NOs will be readmitted. Because the proportion of total patients who are readmitted within 30 days is low, if these models are used to allocate additional health care resources to at-risk patients, most of the patients who receive these resources (~83%) will not actually need them. However, this may be an acceptable sacrifice if it means that more of the patients who really do need these resources will get them. 

In contrast, the non-downsampled random forest model retains a high accuracy and specificity, but has a low sensitivity. This model would be more useful to a hospital with fewer additional resources to allocate. If this model was used to allocate such resources, about 77% of the patients receiving additional services would not need them, which is slightly lower than for the other three models. However, only 11% of the patients that needed such services would actually receive them.

Overall, hospitals seeking to apply machine learning techniques to efficiently allocate additional health care resources to diabetic patients most at-risk of being readmitted to the hospital should consider the balance of sensitivity and specificity of each model, and how it interacts with their available resources. Our analysis suggests that LASSO logistic, downsampled random forest, and downsampled classification tree models are better suited to hospitals with many available resources, while a non-downsampled random forest model could better serve hospitals with fewer resources. 

The predictive accuracy of all these models is satisfactory, but could likely be improved if even more detailed data were available. For example, a patient's readmission risk could be influenced by several variables that were not used in this analysis: a more precise measure of age (not grouped by decades), weight, and indicators of socioeconomic status, just to name a few. However, we were still able to identify a few variables included in the dataset that were most important for determining readmission risk. ELABORATE HERE - WHAT DOES THIS TELL US ABOUT WAYS WE COULD DISTRIBUTE RESOURCES.








References:

(1) https://www.cdc.gov/diabetes/pdfs/data/statistics/national-diabetes-statistics-report.pdf
(2) http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008
(3) https://www.hindawi.com/journals/bmri/2014/781670/



Lisa's instructions:

Final Report
This will highlight some of the new statistical machine learning techniques you learned in this course and also o show that you can communicate the results to a non-statistician. It should read more like a data journalism article than a formal research paper. Here is one good example. You could also look to fivethirtyeight or The Pudding (my personal favorite) for more good examples.

You should make sure to include the following:

Introduce your data and research question. It might be helpful to include a graph/table that summarizes the response variable. Lay out the plan you will use to analyze/model so we (the readers!) can more easily follow along and knows what to expect. It can be helpful to elude to some of the results at the beginning so we know what to watch for. Also mention any important data cleaning decisions you made. You don’t need to tell us EVERYTHING you do, though.

Exploratory work and basic models. Use graphs to illustrate interesting relationshipos that play a role in your final model. DO NOT just show a bunch of graphs because you can. You should label and discuss every graph you include. There is no required number to include. The graphs should be helping us to understand something about your final model and should help us engage more with the data.

Describe the modeling process. I don’t want to know EVERYTHING you tried. You might have tried 10 different things and in the end chose the 8th one. If you think it is important, you can summarize some other methods you tried. But focus on the analysis you found to be most important. This should include at least one of the techniques you learned in this course. Some essentials you will need in the modeling process:

Use rsample() to split the data into a training and test set.
Tune the model using the training data.
Evaluate model using training data (cross-validation or OOB).
Pick a few “best” models and apply them to the test data to decide on the final model.
Summarize the results succinctly. Reiterate why we should be interested in this analysis. Depending on the project, you might do this in different ways. Maybe there were important relationships you learned about. Or maybe you have a nice way to predict something useful. Let us know what we’ve learned and why it’s important/neat/interesting.

Requirements and tips
Your final product will be a knitted html file. The yaml header (at the very top of the .rmd file between the two sets of three dashes) should be similar to the one I have below. You will also submit the .rmd in a separate place. I will likely not look at that file unless there is something I do not understand.
title: "Title"
output:
  html_document:
    df_print: paged
Only include absolutely essential R code! I want to be able to read through the paper nicely. So, most of your R code chunks should have the echo=FALSE option (or do it for all code chunks using my suggestion in the next bullet). The code will still run, but the code is omitted from the document. What is essential? Probably only the code you use to fit models. Anything code for graphs and evaluation can be omitted. Even some of your modeling code might be able to be omitted. When you include code, that means you find it so essential that you should talk about it.

Use other useful code chunk options. You can add results='hide' to also omit the output. This is good for chunks of code you might want to hang on to in case you decide to use it later but don’t want to use in the paper. message=FALSE and warning=FALSE omit any messages and warnings. This link provides some other basic code chunk options. If you want to apply some of these to your entire document, add a chunk similar to the one below. The options you put in there will be applied to all code chunks.

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
Make your R output look nice.

Label graphs (with nice axis labels too) and tables
The kable() function is good to use for tables.
You can change figure widths and heights in code chunk options using fig.width and fig.height.
Always leave an empty row (push enter twice) after an R code chunk. It should prevent your text from showing up next to a graph or table in an odd way.
Be careful if you make any bulleted/numbered lists. Make sure they look the way you expect when knitted. Leaving an empty row between bullets/numbers often helps.
Interpret the R output! You should be describing what it is and what parts of it are interesting.

There is no length requirement. I recommend making it as short as possible while still accomplishing all the required tasks. See the grading rubric on the moodle page (coming soon).